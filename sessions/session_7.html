

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Sesión 7 — Modelización con Machine Learning &mdash; SIQ025 — Programari Professional d&#39;Anàlisi de Dades  documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=e59714d7" />

  
      <script src="../_static/jquery.js?v=5d32c60e"></script>
      <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../_static/documentation_options.js?v=5929fcd5"></script>
      <script src="../_static/doctools.js?v=9bcbadda"></script>
      <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            SIQ025 — Programari Professional d'Anàlisi de Dades
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Sesiones</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="session_1.html">Sesión 1 — Introducción al Análisis de Datos con Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="session_2.html">Sesión 2 — Importación y manipulación de datos</a></li>
<li class="toctree-l1"><a class="reference internal" href="session_3.html">Sesión 3 — Limpieza de datos</a></li>
<li class="toctree-l1"><a class="reference internal" href="session_4.html">Sesión 4 — Preprocesamiento y transformación de datos</a></li>
<li class="toctree-l1"><a class="reference internal" href="session_5.html">Sesión 5 — Visualización de datos (Parte 1)</a></li>
<li class="toctree-l1"><a class="reference internal" href="session_6.html">Sesión 6 — Visualización de datos (Parte 2)</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">SIQ025 — Programari Professional d'Anàlisi de Dades</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Sesión 7 — Modelización con Machine Learning</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/sessions/session_7.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="sesion-7-modelizacion-con-machine-learning">
<h1>Sesión 7 — Modelización con Machine Learning<a class="headerlink" href="#sesion-7-modelizacion-con-machine-learning" title="Link to this heading"></a></h1>
<section id="objetivos">
<h2>Objetivos<a class="headerlink" href="#objetivos" title="Link to this heading"></a></h2>
<p>En esta sesión daremos una visión completa del flujo de <strong>machine learning</strong> aplicado a datos tabulares, tanto en problemas supervisados como no supervisados.</p>
<p>Al finalizar la sesión, serás capaz de:</p>
<ul class="simple">
<li><p>Entender el <strong>pipeline completo</strong>: preprocesamiento → entrenamiento → evaluación.</p></li>
<li><p>Aplicar técnicas de <strong>validación</strong> (train/test y validación cruzada) y razonar sobre <strong>overfitting</strong> y <strong>underfitting</strong>.</p></li>
<li><p>Entrenar y evaluar modelos supervisados de <strong>regresión</strong> y <strong>clasificación</strong>.</p></li>
<li><p>Aplicar <strong>clustering</strong> (K-Means) a problemas de segmentación y exploración de patrones.</p></li>
<li><p>Integrar preprocesamiento y modelo en <strong>pipelines de scikit-learn</strong>.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="introduccion">
<h2>Introducción<a class="headerlink" href="#introduccion" title="Link to this heading"></a></h2>
<p>Hasta ahora hemos trabajado en:</p>
<ul class="simple">
<li><p>Importar y manipular datos (<code class="docutils literal notranslate"><span class="pre">pandas</span></code>).</p></li>
<li><p>Limpiar y preprocesar (valores perdidos, outliers, tipos, escalado y codificación).</p></li>
<li><p>Visualizar y explorar (EDA).</p></li>
</ul>
<p>En esta sesión pasamos a la <strong>modelización</strong>: queremos que un algoritmo aprenda patrones a partir de datos para:</p>
<ul class="simple">
<li><p><strong>Predecir</strong> una variable objetivo (regresión o clasificación).</p></li>
<li><p><strong>Descubrir grupos</strong> o estructuras ocultas (clustering).</p></li>
</ul>
<p>Es importante ver la modelización como parte de un <strong>proceso repetitivo</strong>:</p>
<ol class="arabic simple">
<li><p>Preprocesar datos.</p></li>
<li><p>Entrenar modelo.</p></li>
<li><p>Evaluar.</p></li>
<li><p>Ajustar (hiperparámetros, features, limpieza).</p></li>
<li><p>Volver al paso 1–3 si es necesario.</p></li>
</ol>
</section>
<hr class="docutils" />
<section id="fundamentos-del-pipeline-de-machine-learning">
<h2>Fundamentos del pipeline de Machine Learning<a class="headerlink" href="#fundamentos-del-pipeline-de-machine-learning" title="Link to this heading"></a></h2>
<section id="division-train-test-y-su-importancia">
<h3>División train/test y su importancia<a class="headerlink" href="#division-train-test-y-su-importancia" title="Link to this heading"></a></h3>
<p>Siempre necesitamos evaluar el modelo en <strong>datos que no ha visto en el entrenamiento</strong>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;objetivo&quot;</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s2">&quot;objetivo&quot;</span><span class="p">]</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span>
    <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
    <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span>
    <span class="n">stratify</span><span class="o">=</span><span class="n">y</span>  <span class="c1"># opcional, útil en clasificación</span>
<span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">train</span></code> → se usa para <strong>entrenar</strong> el modelo (ajustar parámetros).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">test</span></code> → se usa solo al final para <strong>evaluar</strong>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">stratify=y</span></code> mantiene proporciones de clases (útil en clasificación).</p></li>
</ul>
<p>Nunca debemos “mirar” los resultados de test para ajustar hiperparámetros de manera sistemática (eso sería fuga de información).</p>
</section>
<section id="validacion-cruzada-cross-validation">
<h3>Validación cruzada (cross-validation)<a class="headerlink" href="#validacion-cruzada-cross-validation" title="Link to this heading"></a></h3>
<p>La <strong>validación cruzada</strong> mejora la estimación del rendimiento dividiendo el conjunto de entrenamiento en varios “folds”.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">cross_val_score</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="kn">import</span> <span class="n">LinearRegression</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s2">&quot;r2&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Scores en cada fold:&quot;</span><span class="p">,</span> <span class="n">scores</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Media R2:&quot;</span><span class="p">,</span> <span class="n">scores</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>
</pre></div>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">cv=5</span></code> → 5 particiones.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">cross_val_score</span></code>:</p>
<ul>
<li><p>Entrena y evalúa en cada fold.</p></li>
<li><p>Devuelve una lista de puntuaciones.</p></li>
</ul>
</li>
</ul>
<p>Esto nos ayuda a detectar si el modelo es muy sensible a la división de los datos.</p>
</section>
<section id="overfitting-y-underfitting">
<h3>Overfitting y underfitting<a class="headerlink" href="#overfitting-y-underfitting" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p><strong>Underfitting</strong>:</p>
<ul>
<li><p>El modelo es <strong>demasiado simple</strong>.</p></li>
<li><p>No captura los patrones de los datos.</p></li>
<li><p>Mal rendimiento en <strong>train y test</strong>.</p></li>
</ul>
</li>
<li><p><strong>Overfitting</strong>:</p>
<ul>
<li><p>El modelo es <strong>demasiado complejo</strong>.</p></li>
<li><p>Aprende ruido y detalles específicos del train.</p></li>
<li><p>Muy buen rendimiento en <strong>train</strong>, peor en <strong>test</strong>.</p></li>
</ul>
</li>
</ul>
<p>Queremos un modelo que tenga un <strong>equilibrio</strong>: lo bastante flexible para aprender, pero sin memorizar el ruido.</p>
<p>Ejemplos típicos:</p>
<ul class="simple">
<li><p>Árbol de decisión muy profundo → riesgo de overfitting.</p></li>
<li><p>Regresión lineal en un problema no lineal → underfitting.</p></li>
</ul>
</section>
<section id="integracion-de-preprocesamiento-modelo-en-un-pipeline">
<h3>Integración de preprocesamiento + modelo en un pipeline<a class="headerlink" href="#integracion-de-preprocesamiento-modelo-en-un-pipeline" title="Link to this heading"></a></h3>
<p>Como en la sesión 4, usamos <code class="docutils literal notranslate"><span class="pre">Pipeline</span></code> y <code class="docutils literal notranslate"><span class="pre">ColumnTransformer</span></code> para asegurar que:</p>
<ul class="simple">
<li><p>Todas las transformaciones (imputación, escalado, codificación) se hacen de forma consistente.</p></li>
<li><p>No hay fuga de información entre train y test.</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.compose</span><span class="w"> </span><span class="kn">import</span> <span class="n">ColumnTransformer</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.pipeline</span><span class="w"> </span><span class="kn">import</span> <span class="n">Pipeline</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">StandardScaler</span><span class="p">,</span> <span class="n">OneHotEncoder</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.impute</span><span class="w"> </span><span class="kn">import</span> <span class="n">SimpleImputer</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.ensemble</span><span class="w"> </span><span class="kn">import</span> <span class="n">RandomForestClassifier</span>

<span class="n">num_cols</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;edad&quot;</span><span class="p">,</span> <span class="s2">&quot;ingresos&quot;</span><span class="p">]</span>
<span class="n">cat_cols</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;ciudad&quot;</span><span class="p">,</span> <span class="s2">&quot;tipo_vivienda&quot;</span><span class="p">]</span>

<span class="n">num_pipe</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([</span>
    <span class="p">(</span><span class="s2">&quot;imputer&quot;</span><span class="p">,</span> <span class="n">SimpleImputer</span><span class="p">(</span><span class="n">strategy</span><span class="o">=</span><span class="s2">&quot;median&quot;</span><span class="p">)),</span>
    <span class="p">(</span><span class="s2">&quot;scaler&quot;</span><span class="p">,</span> <span class="n">StandardScaler</span><span class="p">())</span>
<span class="p">])</span>

<span class="n">cat_pipe</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([</span>
    <span class="p">(</span><span class="s2">&quot;imputer&quot;</span><span class="p">,</span> <span class="n">SimpleImputer</span><span class="p">(</span><span class="n">strategy</span><span class="o">=</span><span class="s2">&quot;most_frequent&quot;</span><span class="p">)),</span>
    <span class="p">(</span><span class="s2">&quot;ohe&quot;</span><span class="p">,</span> <span class="n">OneHotEncoder</span><span class="p">(</span><span class="n">handle_unknown</span><span class="o">=</span><span class="s2">&quot;ignore&quot;</span><span class="p">))</span>
<span class="p">])</span>

<span class="n">preprocessor</span> <span class="o">=</span> <span class="n">ColumnTransformer</span><span class="p">([</span>
    <span class="p">(</span><span class="s2">&quot;num&quot;</span><span class="p">,</span> <span class="n">num_pipe</span><span class="p">,</span> <span class="n">num_cols</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">&quot;cat&quot;</span><span class="p">,</span> <span class="n">cat_pipe</span><span class="p">,</span> <span class="n">cat_cols</span><span class="p">)</span>
<span class="p">])</span>

<span class="n">clf_pipe</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([</span>
    <span class="p">(</span><span class="s2">&quot;preproc&quot;</span><span class="p">,</span> <span class="n">preprocessor</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">&quot;model&quot;</span><span class="p">,</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">))</span>
<span class="p">])</span>

<span class="n">clf_pipe</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">clf_pipe</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="aprendizaje-supervisado-regresion">
<h2>Aprendizaje Supervisado: Regresión<a class="headerlink" href="#aprendizaje-supervisado-regresion" title="Link to this heading"></a></h2>
<p>En problemas de <strong>regresión</strong>, la variable objetivo es numérica (precio, temperatura, ingresos, etc.).</p>
<section id="regresion-lineal-multivariante">
<h3>Regresión lineal multivariante<a class="headerlink" href="#regresion-lineal-multivariante" title="Link to this heading"></a></h3>
<p>La regresión lineal multivariante asume que la relación entre las features y el objetivo es aproximadamente lineal.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">mean_squared_error</span><span class="p">,</span> <span class="n">mean_absolute_error</span><span class="p">,</span> <span class="n">r2_score</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="n">reg</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">reg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">y_pred</span> <span class="o">=</span> <span class="n">reg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="n">mae</span> <span class="o">=</span> <span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="n">rmse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>
<span class="n">r2</span> <span class="o">=</span> <span class="n">r2_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;MAE:&quot;</span><span class="p">,</span> <span class="n">mae</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;RMSE:&quot;</span><span class="p">,</span> <span class="n">rmse</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;R2:&quot;</span><span class="p">,</span> <span class="n">r2</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="metricas-de-regresion">
<h3>Métricas de regresión<a class="headerlink" href="#metricas-de-regresion" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p><strong>MAE (Mean Absolute Error)</strong>:</p>
<ul>
<li><p>Media del valor absoluto de los errores.</p></li>
<li><p>Fácil de interpretar (misma unidad que la variable objetivo).</p></li>
</ul>
</li>
<li><p><strong>RMSE (Root Mean Squared Error)</strong>:</p>
<ul>
<li><p>Raíz cuadrada del error cuadrático medio.</p></li>
<li><p>Penaliza más los errores grandes.</p></li>
</ul>
</li>
<li><p><strong>R² (coeficiente de determinación)</strong>:</p>
<ul>
<li><p>Indica qué proporción de la varianza del objetivo explica el modelo.</p></li>
<li><p>1 → perfecto, 0 → no mejor que predecir la media.</p></li>
</ul>
</li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="aprendizaje-supervisado-clasificacion">
<h2>Aprendizaje Supervisado: Clasificación<a class="headerlink" href="#aprendizaje-supervisado-clasificacion" title="Link to this heading"></a></h2>
<p>En problemas de <strong>clasificación</strong>, la variable objetivo es categórica (ej. “spam/no spam”, “aprobado/suspenso”, tipo de cliente).</p>
<section id="regresion-logistica">
<h3>Regresión logística<a class="headerlink" href="#regresion-logistica" title="Link to this heading"></a></h3>
<p>La <strong>regresión logística</strong> es un modelo lineal para clasificación binaria (y extensiones a multiclase).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">accuracy_score</span><span class="p">,</span> <span class="n">precision_score</span><span class="p">,</span> <span class="n">recall_score</span><span class="p">,</span> <span class="n">f1_score</span>

<span class="n">log_reg</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">max_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="n">log_reg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">y_pred</span> <span class="o">=</span> <span class="n">log_reg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="n">accuracy</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="n">precision</span> <span class="o">=</span> <span class="n">precision_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s2">&quot;binary&quot;</span><span class="p">)</span>  <span class="c1"># o &#39;macro&#39;/&#39;weighted&#39; en multiclase</span>
<span class="n">recall</span> <span class="o">=</span> <span class="n">recall_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s2">&quot;binary&quot;</span><span class="p">)</span>
<span class="n">f1</span> <span class="o">=</span> <span class="n">f1_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s2">&quot;binary&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy:&quot;</span><span class="p">,</span> <span class="n">accuracy</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Precision:&quot;</span><span class="p">,</span> <span class="n">precision</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Recall:&quot;</span><span class="p">,</span> <span class="n">recall</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;F1:&quot;</span><span class="p">,</span> <span class="n">f1</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="k-nearest-neighbors-k-nn">
<h3>k-Nearest Neighbors (k-NN)<a class="headerlink" href="#k-nearest-neighbors-k-nn" title="Link to this heading"></a></h3>
<p>k-NN clasifica una muestra según las clases de sus <strong>k vecinos más cercanos</strong>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.neighbors</span><span class="w"> </span><span class="kn">import</span> <span class="n">KNeighborsClassifier</span>

<span class="n">knn</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">knn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">y_pred_knn</span> <span class="o">=</span> <span class="n">knn</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Sensible a la <strong>escala</strong> de las variables → importante usar escalado (por ejemplo, en un pipeline con <code class="docutils literal notranslate"><span class="pre">StandardScaler</span></code>).</p></li>
<li><p>Hiperparámetro clave: <code class="docutils literal notranslate"><span class="pre">n_neighbors</span></code>.</p></li>
</ul>
</section>
<section id="arboles-de-decision">
<h3>Árboles de decisión<a class="headerlink" href="#arboles-de-decision" title="Link to this heading"></a></h3>
<p>Los <strong>árboles de decisión</strong> dividen recursivamente el espacio de features.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.tree</span><span class="w"> </span><span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>

<span class="n">tree</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">tree</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">y_pred_tree</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Interpretables: podemos visualizar reglas (aunque árboles grandes se vuelven difíciles).</p></li>
<li><p>No requieren escalado de variables.</p></li>
<li><p>Riesgo de overfitting si <code class="docutils literal notranslate"><span class="pre">max_depth</span></code> es muy grande o sin restricciones.</p></li>
</ul>
</section>
<section id="metricas-de-clasificacion-y-matriz-de-confusion">
<h3>Métricas de clasificación y matriz de confusión<a class="headerlink" href="#metricas-de-clasificacion-y-matriz-de-confusion" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p><strong>Accuracy</strong>: proporción de aciertos.</p></li>
<li><p><strong>Precision</strong>: de los que predije como positivos, ¿cuántos lo eran?</p></li>
<li><p><strong>Recall</strong>: de los positivos reales, ¿cuántos detecté?</p></li>
<li><p><strong>F1-score</strong>: media armónica de precision y recall (útil cuando hay clases desbalanceadas).</p></li>
</ul>
<p>Matriz de confusión:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">confusion_matrix</span><span class="p">,</span> <span class="n">ConfusionMatrixDisplay</span>

<span class="n">cm</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="n">disp</span> <span class="o">=</span> <span class="n">ConfusionMatrixDisplay</span><span class="p">(</span><span class="n">confusion_matrix</span><span class="o">=</span><span class="n">cm</span><span class="p">)</span>
<span class="n">disp</span><span class="o">.</span><span class="n">plot</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Matriz de confusión&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p>Interpretación (para binaria):</p>
<ul class="simple">
<li><p>TP (True Positives), FP (False Positives).</p></li>
<li><p>TN (True Negatives), FN (False Negatives).</p></li>
</ul>
<p>Dependiendo del problema, puede interesar más <strong>minimizar falsos positivos</strong> o <strong>falsos negativos</strong>.</p>
</section>
</section>
<hr class="docutils" />
<section id="aprendizaje-no-supervisado-clustering">
<h2>Aprendizaje No Supervisado: Clustering<a class="headerlink" href="#aprendizaje-no-supervisado-clustering" title="Link to this heading"></a></h2>
<p>En <strong>clustering</strong>, no hay etiqueta/objetivo. Queremos agrupar observaciones similares.</p>
<section id="k-means-clustering">
<h3>K-Means clustering<a class="headerlink" href="#k-means-clustering" title="Link to this heading"></a></h3>
<p><code class="docutils literal notranslate"><span class="pre">KMeans</span></code> agrupa los datos en <code class="docutils literal notranslate"><span class="pre">k</span></code> clusters, intentando que los puntos de cada cluster estén lo más cerca posible de su centro.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.cluster</span><span class="w"> </span><span class="kn">import</span> <span class="n">KMeans</span>

<span class="n">X_clust</span> <span class="o">=</span> <span class="n">df</span><span class="p">[[</span><span class="s2">&quot;feature1&quot;</span><span class="p">,</span> <span class="s2">&quot;feature2&quot;</span><span class="p">]]</span>  <span class="c1"># seleccionar variables numéricas</span>

<span class="n">kmeans</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">kmeans</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_clust</span><span class="p">)</span>

<span class="n">labels</span> <span class="o">=</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">labels_</span>          <span class="c1"># cluster asignado a cada muestra</span>
<span class="n">centers</span> <span class="o">=</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">cluster_centers_</span>
</pre></div>
</div>
<p>Visualización en 2D:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_clust</span><span class="p">[</span><span class="s2">&quot;feature1&quot;</span><span class="p">],</span> <span class="n">X_clust</span><span class="p">[</span><span class="s2">&quot;feature2&quot;</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;tab10&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">centers</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">centers</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;X&quot;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Centroides&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;feature1&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;feature2&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;K-Means con 3 clusters&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="eleccion-del-numero-optimo-de-clusters">
<h3>Elección del número óptimo de clusters<a class="headerlink" href="#eleccion-del-numero-optimo-de-clusters" title="Link to this heading"></a></h3>
<p>Dos enfoques habituales:</p>
<ol class="arabic simple">
<li><p><strong>Método del codo (Elbow)</strong>:</p>
<ul class="simple">
<li><p>Calculamos la suma de distancias intra-cluster (<code class="docutils literal notranslate"><span class="pre">inertia_</span></code>) para varios valores de <code class="docutils literal notranslate"><span class="pre">k</span></code>.</p></li>
<li><p>Buscamos un “codo” en la gráfica.</p></li>
</ul>
</li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">inertias</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">K</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">K</span><span class="p">:</span>
    <span class="n">km</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
    <span class="n">km</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_clust</span><span class="p">)</span>
    <span class="n">inertias</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">km</span><span class="o">.</span><span class="n">inertia_</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">inertias</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;o&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Número de clusters k&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Inercia (within-cluster SSE)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Método del codo para elegir k&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<ol class="arabic simple" start="2">
<li><p><strong>Coeficiente de silhouette</strong>:</p>
<ul class="simple">
<li><p>Mide qué tan bien separado está cada punto de otros clusters.</p></li>
<li><p>Valores cercanos a 1 → buena separación; cercanos a 0 → solapamiento; negativos → mala asignación.</p></li>
</ul>
</li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">silhouette_score</span>

<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">7</span><span class="p">):</span>
    <span class="n">km</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
    <span class="n">labels_k</span> <span class="o">=</span> <span class="n">km</span><span class="o">.</span><span class="n">fit_predict</span><span class="p">(</span><span class="n">X_clust</span><span class="p">)</span>
    <span class="n">sil</span> <span class="o">=</span> <span class="n">silhouette_score</span><span class="p">(</span><span class="n">X_clust</span><span class="p">,</span> <span class="n">labels_k</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;k=</span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s2">, silhouette=</span><span class="si">{</span><span class="n">sil</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="interpretacion-y-casos-de-uso">
<h3>Interpretación y casos de uso<a class="headerlink" href="#interpretacion-y-casos-de-uso" title="Link to this heading"></a></h3>
<p>Una vez asignados los clusters:</p>
<ul class="simple">
<li><p>Analizar características medias por cluster (por ejemplo, perfil de cliente):</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">df_clusters</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">df_clusters</span><span class="p">[</span><span class="s2">&quot;cluster&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">labels</span>
<span class="nb">print</span><span class="p">(</span><span class="n">df_clusters</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s2">&quot;cluster&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>
</pre></div>
</div>
<p>Casos de uso típicos:</p>
<ul class="simple">
<li><p><strong>Segmentación de clientes</strong> (por comportamiento de compra, uso de servicios, etc.).</p></li>
<li><p><strong>Detección de patrones</strong> en sensores, datos de uso de aplicaciones, etc.</p></li>
<li><p>Como paso previo para modelos supervisados (por ejemplo, añadir el cluster como feature).</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="ejercicios-sugeridos">
<h2>Ejercicios sugeridos<a class="headerlink" href="#ejercicios-sugeridos" title="Link to this heading"></a></h2>
<ol class="arabic simple">
<li><p><strong>Regresión supervisada</strong>:</p>
<ul class="simple">
<li><p>Tomar un dataset real (por ejemplo, viviendas, precio de coches).</p></li>
<li><p>Entrenar un modelo de <strong>regresión lineal</strong> (posiblemente dentro de un pipeline con preprocesamiento).</p></li>
<li><p>Evaluar con MAE, RMSE y R².</p></li>
</ul>
</li>
<li><p><strong>Clasificación supervisada</strong>:</p>
<ul class="simple">
<li><p>Entrenar al menos <strong>dos modelos de clasificación</strong> (por ejemplo, regresión logística y KNN, o logística y árbol de decisión).</p></li>
<li><p>Comparar métricas: accuracy, precision, recall, F1.</p></li>
<li><p>Analizar la matriz de confusión y comentar errores típicos.</p></li>
</ul>
</li>
<li><p><strong>Clustering (K-Means)</strong>:</p>
<ul class="simple">
<li><p>Aplicar K-Means a un dataset adecuado para <strong>segmentación</strong> (por ejemplo, clientes, alumnos, dispositivos).</p></li>
<li><p>Probar distintos valores de <code class="docutils literal notranslate"><span class="pre">k</span></code> y usar el <strong>método del codo</strong> y/o <strong>silhouette</strong> para justificar una elección.</p></li>
<li><p>Visualizar los clusters resultantes (si es posible en 2D) e interpretar las diferencias entre grupos.</p></li>
</ul>
</li>
<li><p><strong>Pipelines completos</strong>:</p>
<ul class="simple">
<li><p>Integrar en un <code class="docutils literal notranslate"><span class="pre">Pipeline</span></code>:</p>
<ul>
<li><p>Preprocesamiento (imputación, escalado, codificación).</p></li>
<li><p>Modelo (regresión o clasificación).</p></li>
</ul>
</li>
<li><p>Usar validación cruzada para estimar el rendimiento del pipeline.</p></li>
</ul>
</li>
</ol>
</section>
<hr class="docutils" />
<section id="conclusiones-de-la-sesion">
<h2>Conclusiones de la sesión<a class="headerlink" href="#conclusiones-de-la-sesion" title="Link to this heading"></a></h2>
<ul class="simple">
<li><p>La modelización con Machine Learning se basa en un <strong>pipeline completo</strong> donde datos limpios y bien preprocesados son tan importantes como el modelo.</p></li>
<li><p>La <strong>división train/test</strong> y la <strong>validación cruzada</strong> son esenciales para estimar la capacidad de generalización y evitar engañarnos con resultados sobre-entrenados.</p></li>
<li><p>En <strong>regresión</strong>, hemos trabajado con la regresión lineal y métricas como MAE, RMSE y R².</p></li>
<li><p>En <strong>clasificación</strong>, hemos visto modelos básicos (regresión logística, k-NN, árboles de decisión) y métricas como accuracy, precision, recall, F1 y la matriz de confusión.</p></li>
<li><p>En <strong>clustering</strong>, K-Means nos permite descubrir grupos en datos sin etiquetas y es muy útil para segmentación y exploración.</p></li>
<li><p>Integrar preprocesamiento y modelo en <strong>pipelines</strong> facilita la reproducibilidad, evita fugas de información y acerca nuestro trabajo a un entorno profesional.</p></li>
</ul>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright .</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>